{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Amazon Review Classification (Part 4)\n",
    "Author: **Steven Lee**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "\n",
    "# Set pandas display options.\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Read in LDA data.\n",
    "reviews = pd.read_csv(\"../data/reviews_demo.csv\")\n",
    "total_rev = reviews.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Initialise pipeline transformers.\n",
    "sa_pipeline = pipeline('sentiment-analysis')\n",
    "bert_cl_pl = pipeline(\"zero-shot-classification\", model=\"bert-base-cased\")\n",
    "fb_cl_pl = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = ['tool', 'cord', 'floor', 'knife', 'light', 'love', 'glove', 'heavy', 'color', 'used', 'much', 'clamp', \n",
    "                    'worked', 'bulb', 'work', 'wire', 'well', 'blade', 'water', 'door', 'would', 'device', 'quality', 'doe', \n",
    "                    'plastic', 'easy', 'great', 'mouth', 'kitchen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Randomly select a review.\n",
    "rev_idx = sample(range(total_rev), 1)\n",
    "topic, rev_txt, rev_doc, rating = reviews.loc[rev_idx[0], ['dmnt_topic', 'reviewText', 'document', 'overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text: great tools always pick these . THE BEST\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998624324798584}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Review Text: {rev_txt}\")\n",
    "output = sa_pipeline(rev_txt)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Candidate Labels: ['tool', 'cord', 'floor', 'knife', 'light', 'love', 'glove', 'heavy', 'color', 'used', 'much', 'clamp', 'worked', 'bulb', 'work', 'wire', 'well', 'blade', 'water', 'door', 'would', 'device', 'quality', 'doe', 'plastic', 'easy', 'great', 'mouth', 'kitchen']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Topic Candidate Labels: {candidate_labels}\")\n",
    "results_bert = bert_cl_pl(rev_doc, candidate_labels)\n",
    "results_fb = fb_cl_pl(rev_doc, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Model Label-Score Pairs: [('tool', 0.0569), ('knife', 0.0459), ('blade', 0.0448), ('door', 0.0358), ('great', 0.0357), ('would', 0.0352), ('cord', 0.0347), ('device', 0.0346), ('clamp', 0.0346), ('worked', 0.0346)]\n",
      "\n",
      "Bart Model Label-Score Pairs: [('tool', 0.2537), ('great', 0.1504), ('well', 0.1196), ('quality', 0.0906), ('device', 0.0827), ('worked', 0.0682), ('used', 0.0539), ('much', 0.0463), ('would', 0.0397), ('doe', 0.0095)]\n"
     ]
    }
   ],
   "source": [
    "def print_results(results):\n",
    "    labels = results['labels']\n",
    "    scores = results['scores']\n",
    "    output = []\n",
    "\n",
    "    for i in range(10):\n",
    "        pair = ()\n",
    "        pair = (labels[i], round(scores[i], 4))\n",
    "        output.append(pair)\n",
    "        \n",
    "    return output\n",
    "\n",
    "output = print_results(results_bert)\n",
    "print(f\"Bert Model Label-Score Pairs: {output}\")\n",
    "output = print_results(results_fb)\n",
    "print(f\"\\nBart Model Label-Score Pairs: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "1.",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
